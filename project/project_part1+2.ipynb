{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3f4f7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97afa63",
   "metadata": {},
   "source": [
    "connect to the wildfire sqlite database and extract a df from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "63022762",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('FPA_FOD_20170508.sqlite')\n",
    "df = pd.read_sql_query(\"SELECT * from Fires\", conn)[::100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa7790",
   "metadata": {},
   "source": [
    "Turn Date and CONT_DATE to datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e0b81490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(x):\n",
    "    date = datetime.datetime(x['FIRE_YEAR'], 1, 1) + datetime.timedelta(x['DISCOVERY_DOY'] - 1)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8c033121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATE']=df.apply(to_date, axis=1)\n",
    "df['CONT_DATE'] = df.apply(to_date, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61df4dd",
   "metadata": {},
   "source": [
    "create new features denoting the month and day in month the fire was discovered in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "00868397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df['DAY'] = df['DATE'].dt.day\n",
    "df['MONTH'] = df['DATE'].dt.month\n",
    "df['YEAR'] = df['DATE'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf49ea5",
   "metadata": {},
   "source": [
    "create a feature that checks if the fire was discovered around the date of 4th of july"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a25aa8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def july4(x):\n",
    "    if x['MONTH']==7:\n",
    "        if 2<=x['DAY'] and x['DAY']<=6:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ab899ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['JULY4']=df.apply(july4, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423ab183",
   "metadata": {},
   "source": [
    "Adds a features that denote the amount of time passing between fire discovery and fire contaiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "aa5f49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cont_date(x):\n",
    "    if x['CONT_DOY']>0:\n",
    "        date = datetime.datetime(x['FIRE_YEAR'], 1, 1) + datetime.timedelta(x['CONT_DOY'] - 1)\n",
    "        if (date-x['DATE']).days<0:\n",
    "            date = datetime.datetime(x['FIRE_YEAR']+1, 1, 1) + datetime.timedelta(x['CONT_DOY'] - 1)\n",
    "        return date\n",
    "\n",
    "    return datetime.datetime(1990, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e6ce4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cont_date(x):\n",
    "    if x['C_DATE']==datetime.datetime(1990, 1, 1):\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e556b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists_times(x):\n",
    "    if not x['DISCOVERY_TIME'] or not x['CONT_TIME']:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "62727e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hour(x):\n",
    "    if x['DISCOVERY_TIME']:\n",
    "        if int(x['DISCOVERY_TIME'][2:])>30:\n",
    "            return int(x['DISCOVERY_TIME'][:2])+1\n",
    "        return int(x['DISCOVERY_TIME'][:2])\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "70aeeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hour_do_dis_date(x):\n",
    "    total = x['DATE']\n",
    "    if x['HOUR']>=0:\n",
    "        total += datetime.timedelta(hours = int(x['DISCOVERY_TIME'][:2]), minutes=int(x['DISCOVERY_TIME'][2:]))\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a06c4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cont_hour(x):\n",
    "    if x['CONT_TIME']:\n",
    "        if int(x['CONT_TIME'][2:])>30:\n",
    "            return int(x['CONT_TIME'][:2])+1\n",
    "        return int(x['CONT_TIME'][:2])\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6c1b56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hour_to_cont_date(x):\n",
    "    if x['C_DATE']==datetime.datetime(1990, 1, 1):\n",
    "        return x['C_DATE']\n",
    "    total = x['C_DATE']\n",
    "    if x['C_HOUR']>=0:\n",
    "        total += datetime.timedelta(hours = int(x['CONT_TIME'][:2]), minutes=int(x['CONT_TIME'][2:]))\n",
    "    return total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3a239a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration(x):\n",
    "    if x['C_DATE']==datetime.datetime(1990, 1, 1):\n",
    "        return -1\n",
    "    else:\n",
    "        if x['EXISTS_TIMES']==1:\n",
    "            difference = x['C_DATE']-x['DATE']\n",
    "        else:\n",
    "            difference = (x['C_DATE']+ datetime.timedelta(hours = 23, minutes=59))-x['DATE']  \n",
    "        return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "be9c8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_days(x):\n",
    "    if x['DURATION']==-1:\n",
    "        return -1\n",
    "    return x['DURATION'].days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1b9fafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_hours(x):\n",
    "    if x['DURATION']==-1:\n",
    "        return -1\n",
    "    return x['DURATION'].days*24+x['DURATION'].seconds//3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "11e2dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_30m(x):\n",
    "    if x['DURATION']==-1:\n",
    "        return 0\n",
    "    if x['DURATION']== datetime.timedelta(hours = 0, minutes=30):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8cd0d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_1h(x):\n",
    "    if x['DURATION']==-1:\n",
    "        return 0\n",
    "    if x['DURATION']== datetime.timedelta(hours = 1, minutes=0):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "38ab4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_same_d(x):\n",
    "    if x['DURATION_DAYS']==-1:\n",
    "        return 0\n",
    "    if x['DURATION_DAYS']==0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "efde2df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['C_DATE'] = df.apply(to_cont_date, axis=1)\n",
    "df['C_DATE_EXISTS'] = df.apply(is_cont_date, axis=1)\n",
    "df['C_DATE'] = pd.to_datetime(df['C_DATE'])\n",
    "df['EXISTS_TIMES']=df.apply(exists_times, axis=1)\n",
    "df['HOUR']=df.apply(add_hour, axis=1)\n",
    "df['DATE']=df.apply(add_hour_do_dis_date, axis=1)\n",
    "df['C_HOUR']=df.apply(add_cont_hour, axis=1)\n",
    "df['C_DATE']=df.apply(add_hour_to_cont_date, axis=1)\n",
    "df['DURATION']=df.apply(duration, axis=1)\n",
    "df['DURATION_DAYS']=df.apply(duration_days, axis=1)\n",
    "df['DURATION_HOURS']=df.apply(duration_hours, axis=1)\n",
    "df['DUR_30M']=df.apply(duration_30m, axis=1)\n",
    "df['DUR_1H']=df.apply(duration_1h, axis=1)\n",
    "df['DUR_1D']=df.apply(duration_same_d, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847e48b",
   "metadata": {},
   "source": [
    "Drop all features that may cause leakage or are indexes in databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bafce5e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['C_DATE' 'DURATION'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9596/3787186932.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m df_dropped = df.drop(columns=[\"FPA_ID\", \"SOURCE_SYSTEM_TYPE\", \"SOURCE_SYSTEM\", \"NWCG_REPORTING_AGENCY\",\n\u001B[0m\u001B[0;32m      2\u001B[0m                              \u001B[1;34m\"NWCG_REPORTING_UNIT_ID\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"NWCG_REPORTING_UNIT_NAME\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"SOURCE_REPORTING_UNIT\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m                              \u001B[1;34m\"SOURCE_REPORTING_UNIT_NAME\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Shape\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"OBJECTID\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"LOCAL_FIRE_REPORT_ID\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m                              \u001B[1;34m\"LOCAL_INCIDENT_ID\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"FIRE_CODE\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"FIRE_NAME\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"ICS_209_INCIDENT_NUMBER\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m                              \u001B[1;34m\"ICS_209_NAME\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"MTBS_ID\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"MTBS_FIRE_NAME\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"COMPLEX_NAME\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"FIPS_CODE\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"FIPS_NAME\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36mdrop\u001B[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[0;32m   4904\u001B[0m                 \u001B[0mweight\u001B[0m  \u001B[1;36m1.0\u001B[0m     \u001B[1;36m0.8\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4905\u001B[0m         \"\"\"\n\u001B[1;32m-> 4906\u001B[1;33m         return super().drop(\n\u001B[0m\u001B[0;32m   4907\u001B[0m             \u001B[0mlabels\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4908\u001B[0m             \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36mdrop\u001B[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[0;32m   4148\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;32min\u001B[0m \u001B[0maxes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4149\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4150\u001B[1;33m                 \u001B[0mobj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_drop_axis\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4151\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4152\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m_drop_axis\u001B[1;34m(self, labels, axis, level, errors)\u001B[0m\n\u001B[0;32m   4183\u001B[0m                 \u001B[0mnew_axis\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4184\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4185\u001B[1;33m                 \u001B[0mnew_axis\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4186\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreindex\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0maxis_name\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mnew_axis\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4187\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mdrop\u001B[1;34m(self, labels, errors)\u001B[0m\n\u001B[0;32m   6015\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0many\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6016\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0merrors\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;34m\"ignore\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6017\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{labels[mask]} not found in axis\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   6018\u001B[0m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m~\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6019\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdelete\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['C_DATE' 'DURATION'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df_dropped = df.drop(columns=[\"FPA_ID\", \"SOURCE_SYSTEM_TYPE\", \"SOURCE_SYSTEM\", \"NWCG_REPORTING_AGENCY\",\n",
    "                             \"NWCG_REPORTING_UNIT_ID\", \"NWCG_REPORTING_UNIT_NAME\", \"SOURCE_REPORTING_UNIT\",\n",
    "                             \"SOURCE_REPORTING_UNIT_NAME\", \"Shape\", \"OBJECTID\", \"LOCAL_FIRE_REPORT_ID\",\n",
    "                             \"LOCAL_INCIDENT_ID\", \"FIRE_CODE\", \"FIRE_NAME\", \"ICS_209_INCIDENT_NUMBER\",\n",
    "                             \"ICS_209_NAME\", \"MTBS_ID\", \"MTBS_FIRE_NAME\", \"COMPLEX_NAME\", \"FIPS_CODE\", \"FIPS_NAME\",\n",
    "                             \"OWNER_CODE\", \"DATE\", \"FIRE_YEAR\", \"DISCOVERY_DATE\", \"DISCOVERY_TIME\", \n",
    "                             \"CONT_DATE\", \"CONT_DOY\", \"CONT_TIME\", \"C_DATE\", \"DURATION\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b9abc",
   "metadata": {},
   "source": [
    "Turn the dataframe into a GeoDataFrame to work with geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0408a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = gpd.GeoDataFrame(df_dropped, geometry=gpd.points_from_xy(df_dropped.LONGITUDE, df_dropped.LATITUDE), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0304f4",
   "metadata": {},
   "source": [
    "A function that gets a path to a csv containing a geometry column and turns it into a geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bda38122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gdf(file_path):\n",
    "        gdf = pd.read_csv(file_path)\n",
    "        return gpd.GeoDataFrame(gdf.loc[:, [c for c in gdf.columns if c != \"geometry\"]], \n",
    "                                geometry=gpd.GeoSeries.from_wkt(gdf[\"geometry\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4699a684",
   "metadata": {},
   "source": [
    "get distance of fire from railroad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "68cc690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_to_rails_feature(df):\n",
    "    gdf = get_gdf(\"datasets/rail_north_america/rails_geo.csv\")\n",
    "\n",
    "    gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "    gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "    df.crs = gdf.crs\n",
    "    res = gpd.sjoin_nearest(df, gdf, distance_col=\"distance_to_rails\", how=\"left\")\n",
    "    res = res.drop([\"index_right\", \"scalerank\", \"featurecla\", \"sov_a3\", \"uident\", \"add\", \"natrlscale\", \"continent\"], axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516c287",
   "metadata": {},
   "source": [
    "getting population density per location of fire feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "63a75156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pop(df):\n",
    "    gdf = get_gdf(\"datasets/population_density/data_populations_usa.csv\")\n",
    "    gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "    gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "    \n",
    "    res = gpd.sjoin_nearest(df, gdf, how=\"left\")\n",
    "    res = res.drop([\"index_right\"], axis=1)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab53df2",
   "metadata": {},
   "source": [
    "A general function that gets a dataframe a file_path to a dataframe with geometry column and adds to the df the columns of the nearest point in the geodataframe with a feature for distance from that point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "eb9b302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_feature(df, file_path, feature_name):\n",
    "    gdf = get_gdf(file_path)\n",
    "\n",
    "    gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "    gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "    df.crs = gdf.crs\n",
    "\n",
    "    res = gpd.sjoin_nearest(df, gdf, distance_col=feature_name, how=\"left\")\n",
    "    res = res.drop([\"OBJECTID\", \"index_right\"], axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a5f32",
   "metadata": {},
   "source": [
    "get the distance from nearest city to the fire and the name of that city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fc9c4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_and_distance_feature(df, file_path, feature_name):\n",
    "    gdf = get_gdf(file_path)\n",
    "\n",
    "    gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "    gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "    df.crs = gdf.crs\n",
    "\n",
    "    res = gpd.sjoin_nearest(df, gdf, distance_col=feature_name, how=\"left\")\n",
    "    res = res.drop([\"index_right\"], axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4ccce",
   "metadata": {},
   "source": [
    "adds the average, min, max, prcp and pan evaporation features in the area of the fire before the fire started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "11d9e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_check(row):\n",
    "    day_to_check = row['DISCOVERY_DOY'] - (1 + (row['DISCOVERY_DOY'] - 1) % 3)\n",
    "    date = datetime.datetime(row['YEAR'], 1, 1) + datetime.timedelta(int(day_to_check))\n",
    "    \n",
    "    if date.year < 1992:\n",
    "        return 1992, 1, 1\n",
    "    else:\n",
    "        return date.year, date.month, date.day\n",
    "\n",
    "def temps_func(df):\n",
    "    gdf = pd.read_csv(\"datasets/temps_dfs/temps_area_codes.csv\")\n",
    "    gdf = gpd.GeoDataFrame(gdf.loc[:, [c for c in gdf.columns if c != \"geometry\"]],\n",
    "                           geometry=gpd.GeoSeries.from_wkt(gdf[\"geometry\"]))\n",
    "    \n",
    "    gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "    gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "    df.crs = gdf.crs\n",
    "\n",
    "    df = gpd.sjoin_nearest(df, gdf, how=\"left\")\n",
    "    \n",
    "    df = df.drop([\"index_right\"], axis=1)\n",
    "\n",
    "    df[\"day_remove\"] = df[\"DAY\"]\n",
    "    df[\"month_remove\"] = df[\"MONTH\"]\n",
    "    df[\"year_remove\"] = df[\"YEAR\"]\n",
    "\n",
    "    dates = np.array(list(df.apply(date_to_check, axis=1)))\n",
    "\n",
    "    df[\"DAY\"] = dates[:, 2]\n",
    "    df[\"MONTH\"] = dates[:, 1]\n",
    "    df[\"YEAR\"] = dates[:, 0]\n",
    "\n",
    "    gdf = pd.read_csv(f\"datasets/temps_dfs/temps_area_code_dates.csv\")\n",
    "\n",
    "    gdf[\"YEAR\"] = gdf[\"YEAR\"].astype(\"int32\")\n",
    "    df[\"YEAR\"] = df[\"YEAR\"].astype(\"int32\")\n",
    "    gdf[\"MONTH\"] = gdf[\"MONTH\"].astype(\"int32\")\n",
    "    df[\"MONTH\"] = df[\"MONTH\"].astype(\"int32\")\n",
    "    gdf[\"DAY\"] = gdf[\"DAY\"].astype(\"int32\")\n",
    "    df[\"DAY\"] = df[\"DAY\"].astype(\"int32\")\n",
    "\n",
    "    df = pd.merge(df, gdf, on=[\"DAY\", \"MONTH\", \"YEAR\", \"area_code\"], how=\"left\")\n",
    "\n",
    "    df[\"DAY\"] = df[\"day_remove\"]\n",
    "    df[\"MONTH\"] = df[\"month_remove\"]\n",
    "    df[\"YEAR\"] = df[\"year_remove\"]\n",
    "    df = df.drop([\"area_code\", \"day_remove\", \"month_remove\", \"year_remove\"], axis=1)\n",
    "    df = df.drop_duplicates(subset=[\"FOD_ID\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d881c033",
   "metadata": {},
   "source": [
    "adds demographic features and smoking stats per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bd09ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"Virgin Islands\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "    \"United States\": \"United States\"\n",
    "}\n",
    "\n",
    "def map_states(y):\n",
    "    return us_state_to_abbrev.get(y, \"____\")\n",
    "\n",
    "# adds data from https://www.kff.org/statedata/\n",
    "def get_info_by_states(df, address):\n",
    "    df = df.sort_values(by=[\"YEAR\"])\n",
    "    df[\"YEAR\"] = pd.to_numeric(df[\"YEAR\"])\n",
    "\n",
    "    csv_files = ['citenzship',\n",
    "                 'race',\n",
    "                 'tobacco',\n",
    "                 'demographics_with_years']\n",
    "    \n",
    "    for table in csv_files:\n",
    "        curr = pd.read_csv(f\"{address}/{table}.csv\")\n",
    "        curr = curr.dropna()\n",
    "        if table == 'tobacco':\n",
    "            curr.rename(columns = {'State':'STATE', \"Year\":\"YEAR\"}, inplace = True)\n",
    "        else:\n",
    "            curr.rename(columns = {'Location':'STATE'}, inplace = True)\n",
    "            \n",
    "        curr['STATE'] = curr['STATE'].apply(map_states)\n",
    "        \n",
    "        if (table == 'tobacco') or (table == \"demographics_with_years\"):\n",
    "            curr[\"YEAR\"] = pd.to_numeric(curr[\"YEAR\"])\n",
    "            curr = curr.sort_values(by=[\"YEAR\"])\n",
    "            \n",
    "            df = pd.merge_asof(df, curr, on=\"YEAR\", by='STATE', direction=\"nearest\")\n",
    "            \n",
    "        else:\n",
    "            df = pd.merge(df, curr, on=['STATE'], how=\"left\")\n",
    "    \n",
    "    df = df.drop_duplicates(subset=[\"FOD_ID\"])\n",
    "    df = df.applymap(lambda x: x.strip('%') if isinstance(x, str) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b37b91",
   "metadata": {},
   "source": [
    "turns the fire class feature into an ordinal feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "28e95b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_fire_class_to_ordinal(row):\n",
    "    if row[\"FIRE_SIZE_CLASS\"] == \"A\":\n",
    "        return 0\n",
    "    elif row[\"FIRE_SIZE_CLASS\"] == \"B\":\n",
    "        return 1\n",
    "    elif row[\"FIRE_SIZE_CLASS\"] == \"C\":\n",
    "        return 2\n",
    "    elif row[\"FIRE_SIZE_CLASS\"] == \"D\":\n",
    "        return 3\n",
    "    elif row[\"FIRE_SIZE_CLASS\"] == \"E\":\n",
    "        return 4\n",
    "    elif row[\"FIRE_SIZE_CLASS\"] == \"F\":\n",
    "        return 5\n",
    "    elif row[\"FIRE_SIZE_CLASS\"] == \"G\":\n",
    "        return 6\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e7d825",
   "metadata": {},
   "source": [
    "a function that checks if PCA based features help the model diffrintiate between sampeles, in our testing it seems tha pca features caused overfit and as such we do not use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "814ef7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pca_features(df):\n",
    "    features = [i for i in df.columns if i not in ['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR']]\n",
    "\n",
    "    for i in range(2, 21):\n",
    "        df_with_embd = df.copy()\n",
    "        for j in range(2):\n",
    "\n",
    "            if j == 0:\n",
    "                pca = PCA(n_components=i)\n",
    "            else:\n",
    "                pca = PCA(n_components=i, whiten=True)\n",
    "\n",
    "            embd = pca.fit_transform(df[features])\n",
    "\n",
    "            for k in range(i):\n",
    "                df_with_embd[f\"embd{k}\"] = embd[:, k]\n",
    "\n",
    "            X = df_with_embd.drop(columns=['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR'])\n",
    "            y = df_with_embd['STAT_CAUSE_DESCR']\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "            model = RandomForestClassifier(n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            preds = model.predict(X_test)\n",
    "\n",
    "            cr_dict = classification_report(y_test, preds, output_dict=True)\n",
    "            accuracy = cr_dict[\"accuracy\"]\n",
    "            macro_f1 = cr_dict[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "            if j == 0:\n",
    "                print(f\"with {i} components accuracy: {accuracy} and macro f1: {macro_f1}\")\n",
    "            else:\n",
    "                print(f\"with {i} and whiten components accuracy: {accuracy} and macro f1: {macro_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58052876",
   "metadata": {},
   "source": [
    "a function that gets a dataframe and adds features to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4ab80a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    df = df.to_crs(\"EPSG:3857\")\n",
    "    print(\"getting pop feature...\", end=\"\")\n",
    "    df = get_pop(df)\n",
    "    df = df.groupby(\"FOD_ID\").first().reset_index()\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"getting rails feature...\", end=\"\")\n",
    "    df = get_distance_to_rails_feature(df)\n",
    "    df = df.groupby(\"FOD_ID\").first().reset_index()\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"getting powerline feature...\", end=\"\")\n",
    "    df = get_distance_feature(df, \"datasets/powerlines/powerlines.csv\", \"distance_to_powerline\")\n",
    "    df = df.groupby(\"FOD_ID\").first().reset_index()\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"getting landfill feature...\", end=\"\")\n",
    "    df = get_distance_feature(df, \"datasets/landfill_locations/Landfill_Locations.csv\", \"distance_to_landfill\")\n",
    "    df = df.groupby(\"FOD_ID\").first().reset_index()\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"getting home_parks_distance feature...\", end=\"\")\n",
    "    df = get_distance_feature(df, \"datasets/mobile_home_parks/Mobile_Home_Parks.csv\", \"home_parks_distance\")\n",
    "    df = df.groupby(\"FOD_ID\").first().reset_index()\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"getting public school distance feature...\", end=\"\")\n",
    "    df = get_distance_feature(df, \"datasets/schools/Public_Schools.csv\", \"public_school_distance\")\n",
    "    df = df.groupby(\"FOD_ID\").first().reset_index()\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"getting city and distance feature...\", end=\"\")\n",
    "    df = get_city_and_distance_feature(df, \"datasets/cities/CityBoundaries.csv\", \"city_distance\")\n",
    "    df = df.groupby(\"FOD_ID\").first().reset_index()\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"getting temperature features...\", end=\"\")\n",
    "    df = temps_func(df)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"getting ordinal fire class...\", end=\"\")\n",
    "    df[\"fire_class_ordinal\"] = df.apply(turn_fire_class_to_ordinal, axis=1)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"getting usa by state features...\", end=\"\")\n",
    "    df = get_info_by_states(df, \"datasets/usa_by_state\")\n",
    "    print(\"Done\")\n",
    "    \n",
    "    df[\"max_temp\"].fillna(value=df[\"max_temp\"].mean(), inplace=True)\n",
    "    df[\"min_temp\"].fillna(value=df[\"min_temp\"].mean(), inplace=True)\n",
    "    df[\"prcp\"].fillna(value=df[\"prcp\"].mean(), inplace=True)\n",
    "    df[\"Pan_evaporation\"].fillna(value=df[\"Pan_evaporation\"].mean(), inplace=True)\n",
    "    df[\"avg_temp\"].fillna(value=df[\"avg_temp\"].mean(), inplace=True)\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770962d",
   "metadata": {},
   "source": [
    "A function that gets a geodataframe and adds the city name feature to ready it for the one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7d1db75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_ready_for_encoder(df):\n",
    "    print(\"getting city and distance feature...\", end=\"\")\n",
    "    df = get_city_and_distance_feature(df, \"datasets/cities/CityBoundaries.csv\", \"city_distance\")\n",
    "    df = df.groupby(\"FOD_ID\").first().reset_index()\n",
    "    print(\"Done\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0f8e57f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting city and distance feature..."
     ]
    },
    {
     "ename": "ProjError",
     "evalue": "x, y, z, and time must be same size",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mProjError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9596/1512466247.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf_dropped\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_df_ready_for_encoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_dropped\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9596/2955552509.py\u001B[0m in \u001B[0;36mget_df_ready_for_encoder\u001B[1;34m(df)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mget_df_ready_for_encoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"getting city and distance feature...\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_city_and_distance_feature\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"datasets/cities/CityBoundaries.csv\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"city_distance\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m     \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"FOD_ID\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfirst\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Done\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9596/3660506600.py\u001B[0m in \u001B[0;36mget_city_and_distance_feature\u001B[1;34m(df, file_path, feature_name)\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mgdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcrs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"EPSG:4326\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mgdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_crs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"EPSG:3857\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m     \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcrs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcrs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\geopandas\\geodataframe.py\u001B[0m in \u001B[0;36mto_crs\u001B[1;34m(self, crs, epsg, inplace)\u001B[0m\n\u001B[0;32m   1362\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1363\u001B[0m             \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1364\u001B[1;33m         \u001B[0mgeom\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgeometry\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_crs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepsg\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepsg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1365\u001B[0m         \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgeometry\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgeom\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1366\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\geopandas\\geoseries.py\u001B[0m in \u001B[0;36mto_crs\u001B[1;34m(self, crs, epsg)\u001B[0m\n\u001B[0;32m   1122\u001B[0m         \"\"\"\n\u001B[0;32m   1123\u001B[0m         return GeoSeries(\n\u001B[1;32m-> 1124\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_crs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepsg\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepsg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1125\u001B[0m         )\n\u001B[0;32m   1126\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\geopandas\\array.py\u001B[0m in \u001B[0;36mto_crs\u001B[1;34m(self, crs, epsg)\u001B[0m\n\u001B[0;32m    777\u001B[0m         \u001B[0mtransformer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTransformer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_crs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malways_xy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 779\u001B[1;33m         \u001B[0mnew_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvectorized\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransformer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    780\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mGeometryArray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnew_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcrs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    781\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\geopandas\\_vectorized.py\u001B[0m in \u001B[0;36mtransform\u001B[1;34m(data, func)\u001B[0m\n\u001B[0;32m   1112\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1113\u001B[0m         \u001B[0mcoords_z\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_coordinates\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mhas_z\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minclude_z\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1114\u001B[1;33m         \u001B[0mnew_coords_z\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcoords_z\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcoords_z\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcoords_z\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1115\u001B[0m         \u001B[0mresult\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mhas_z\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mset_coordinates\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mhas_z\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnew_coords_z\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mT\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1116\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pyproj\\transformer.py\u001B[0m in \u001B[0;36mtransform\u001B[1;34m(self, xx, yy, zz, tt, radians, errcheck, direction)\u001B[0m\n\u001B[0;32m    428\u001B[0m             \u001B[0mintime\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    429\u001B[0m         \u001B[1;31m# call pj_transform.  inx,iny,inz buffers modified in place.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 430\u001B[1;33m         self._transformer._transform(\n\u001B[0m\u001B[0;32m    431\u001B[0m             \u001B[0minx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    432\u001B[0m             \u001B[0miny\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpyproj/_transformer.pyx\u001B[0m in \u001B[0;36mpyproj._transformer._Transformer._transform\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mProjError\u001B[0m: x, y, z, and time must be same size"
     ]
    }
   ],
   "source": [
    "df_dropped = get_df_ready_for_encoder(df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1774e2f",
   "metadata": {},
   "source": [
    "one hot encode the STATE OWNER_DESCR and NAME features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "encoder.fit(df_dropped[[\"STATE\", \"OWNER_DESCR\", \"NAME\"]])\n",
    "COLUMNS_NAME_ONE_HOT = list(encoder.categories_[0]) + list(encoder.categories_[1]) + list(encoder.categories_[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1f827",
   "metadata": {},
   "source": [
    "The general preprocessing function that receives a dataframe from the wildifre dataset and preprocesses it for the use of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4566547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, encoder):\n",
    "    print(\"working on dates...\", end=\"\")\n",
    "    df['DATE']=df.apply(to_date, axis=1)\n",
    "    df['CONT_DATE'] = df.apply(to_date, axis=1)\n",
    "    \n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['DAY'] = df['DATE'].dt.day\n",
    "    df['MONTH'] = df['DATE'].dt.month\n",
    "    df['YEAR'] = df['DATE'].dt.year\n",
    "    \n",
    "    df['JULY4']=df.apply(july4, axis=1)\n",
    "    \n",
    "    df['C_DATE'] = df.apply(to_cont_date, axis=1)\n",
    "    df['C_DATE_EXISTS'] = df.apply(is_cont_date, axis=1)\n",
    "    df['C_DATE'] = pd.to_datetime(df['C_DATE'])\n",
    "    df['EXISTS_TIMES']=df.apply(exists_times, axis=1)\n",
    "    df['HOUR']=df.apply(add_hour, axis=1)\n",
    "    df['DATE']=df.apply(add_hour_do_dis_date, axis=1)\n",
    "    df['C_HOUR']=df.apply(add_cont_hour, axis=1)\n",
    "    df['C_DATE']=df.apply(add_hour_to_cont_date, axis=1)\n",
    "    df['DURATION']=df.apply(duration, axis=1)\n",
    "    df['DURATION_DAYS']=df.apply(duration_days, axis=1)\n",
    "    df['DURATION_HOURS']=df.apply(duration_hours, axis=1)\n",
    "    df['DUR_30M']=df.apply(duration_30m, axis=1)\n",
    "    df['DUR_1H']=df.apply(duration_1h, axis=1)\n",
    "    df['DUR_1D']=df.apply(duration_same_d, axis=1)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    df = df.drop(columns=[\"FPA_ID\", \"SOURCE_SYSTEM_TYPE\", \"SOURCE_SYSTEM\", \"NWCG_REPORTING_AGENCY\",\n",
    "                         \"NWCG_REPORTING_UNIT_ID\", \"NWCG_REPORTING_UNIT_NAME\", \"SOURCE_REPORTING_UNIT\",\n",
    "                         \"SOURCE_REPORTING_UNIT_NAME\", \"Shape\", \"OBJECTID\", \"LOCAL_FIRE_REPORT_ID\",\n",
    "                         \"LOCAL_INCIDENT_ID\", \"FIRE_CODE\", \"FIRE_NAME\", \"ICS_209_INCIDENT_NUMBER\",\n",
    "                         \"ICS_209_NAME\", \"MTBS_ID\", \"MTBS_FIRE_NAME\", \"COMPLEX_NAME\", \"FIPS_CODE\", \"FIPS_NAME\",\n",
    "                         \"OWNER_CODE\", \"DATE\", \"FIRE_YEAR\", \"DISCOVERY_DATE\", \"DISCOVERY_TIME\", \n",
    "                         \"CONT_DATE\", \"CONT_DOY\", \"CONT_TIME\", \"C_DATE\", \"DURATION\"])\n",
    "    \n",
    "    df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE), crs=\"EPSG:4326\")\n",
    "    \n",
    "    df = get_features(df)\n",
    "    \n",
    "    df = df.rename(columns={\"z\": \"population density\"})\n",
    "    df = df.drop(columns=[\"FOD_ID\", \"geometry\"])\n",
    "    \n",
    "    print(\"One hot encoding...\", end=\"\")\n",
    "    one_hots = pd.DataFrame(encoder.transform(df[[\"STATE\", \"OWNER_DESCR\", \"NAME\"]]),\n",
    "                            index=df.index, columns=COLUMNS_NAME_ONE_HOT)\n",
    "    print(\"Done\")\n",
    "\n",
    "    df = pd.concat([df, one_hots], axis=1)\n",
    "\n",
    "    df = df.drop(columns=[\"COUNTY\", \"STATE\", \"FIRE_SIZE_CLASS\", \"OWNER_DESCR\", \"NAME\", \"Unnamed: 0\"])\n",
    "    \n",
    "    features = [i for i in df.columns if i not in ['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR']]\n",
    "    df[features] = df[features].astype(\"float\")\n",
    "    \n",
    "    print(\"Interaction of smokers and population size...\", end=\"\")\n",
    "    df[\"Smoke everyday number\"] = df[\"Smoke everyday\"] * df[\"Total\"]\n",
    "    df[\"Smoke some days number\"] = df[\"Smoke some days\"] * df[\"Total\"]\n",
    "    df[\"Former smoker number\"] = df[\"Former smoker\"] * df[\"Total\"]\n",
    "    df[\"Never smoked number\"] = df[\"Never smoked\"] * df[\"Total\"]\n",
    "    print(\"Done\")\n",
    "    \n",
    "    features = [i for i in df.columns if i not in ['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR']]\n",
    "    \n",
    "    print(\"Normalizing data...\", end=\"\")\n",
    "    scaler = StandardScaler()\n",
    "    df_normalized = pd.DataFrame(scaler.fit_transform(df[features]), columns=[features])\n",
    "    df[features] = df_normalized\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}