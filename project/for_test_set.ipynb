{"cells":[{"cell_type":"code","execution_count":null,"id":"01eb2c23","metadata":{"id":"01eb2c23"},"outputs":[],"source":["import sqlite3\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import classification_report, f1_score\n","import datetime\n","import geopandas as gpd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.metrics import confusion_matrix\n","from sklearn.ensemble import RandomForestClassifier\n","from imblearn.over_sampling import RandomOverSampler"]},{"cell_type":"markdown","id":"9b3bc453","metadata":{"id":"9b3bc453"},"source":["To run this file with test set please load in the block below your data and run the entire notebook, classification report will be printed in the end of the notebook"]},{"cell_type":"code","execution_count":null,"id":"f7f1e714","metadata":{"id":"f7f1e714"},"outputs":[],"source":["# conn = sqlite3.connect('FPA_FOD_20170508.sqlite')\n","# test_df = pd.read_sql_query(\"SELECT * from Fires\", conn) # <- example for loading data\n","\n","test_df = None  # todo: please load test data here as a dataframe"]},{"cell_type":"code","execution_count":null,"id":"353014ca","metadata":{"id":"353014ca"},"outputs":[],"source":["def to_date(x):\n","    date = datetime.datetime(x['FIRE_YEAR'], 1, 1) + datetime.timedelta(x['DISCOVERY_DOY'] - 1)\n","    return date"]},{"cell_type":"code","execution_count":null,"id":"73eb956a","metadata":{"id":"73eb956a"},"outputs":[],"source":["def july4(x):\n","    if x['MONTH']==7:\n","        if 2<=x['DAY'] and x['DAY']<=6:\n","            return 1\n","    return 0"]},{"cell_type":"code","execution_count":null,"id":"f1095ac9","metadata":{"id":"f1095ac9"},"outputs":[],"source":["def to_cont_date(x):\n","    if x['CONT_DOY']>0:\n","        date = datetime.datetime(x['FIRE_YEAR'], 1, 1) + datetime.timedelta(x['CONT_DOY'] - 1)\n","        if (date-x['DATE']).days<0:\n","            date = datetime.datetime(x['FIRE_YEAR']+1, 1, 1) + datetime.timedelta(x['CONT_DOY'] - 1)\n","        return date\n","\n","    return datetime.datetime(1990, 1, 1)"]},{"cell_type":"code","execution_count":null,"id":"7e37ba2e","metadata":{"id":"7e37ba2e"},"outputs":[],"source":["def is_cont_date(x):\n","    if x['C_DATE']==datetime.datetime(1990, 1, 1):\n","        return 0\n","    return 1"]},{"cell_type":"code","execution_count":null,"id":"9acde51f","metadata":{"id":"9acde51f"},"outputs":[],"source":["def exists_times(x):\n","    if not x['DISCOVERY_TIME'] or not x['CONT_TIME']:\n","        return 0\n","    return 1"]},{"cell_type":"code","execution_count":null,"id":"dfa8f708","metadata":{"id":"dfa8f708"},"outputs":[],"source":["def add_hour(x):\n","    if x['DISCOVERY_TIME']:\n","        if int(x['DISCOVERY_TIME'][2:])>30:\n","            return int(x['DISCOVERY_TIME'][:2])+1\n","        return int(x['DISCOVERY_TIME'][:2])\n","    return -1"]},{"cell_type":"code","execution_count":null,"id":"83d85799","metadata":{"id":"83d85799"},"outputs":[],"source":["def add_hour_do_dis_date(x):\n","    total = x['DATE']\n","    if x['HOUR']>=0:\n","        total += datetime.timedelta(hours = int(x['DISCOVERY_TIME'][:2]), minutes=int(x['DISCOVERY_TIME'][2:]))\n","    return total"]},{"cell_type":"code","execution_count":null,"id":"50c5cc40","metadata":{"id":"50c5cc40"},"outputs":[],"source":["def add_cont_hour(x):\n","    if x['CONT_TIME']:\n","        if int(x['CONT_TIME'][2:])>30:\n","            return int(x['CONT_TIME'][:2])+1\n","        return int(x['CONT_TIME'][:2])\n","    return -1"]},{"cell_type":"code","execution_count":null,"id":"cbc0517a","metadata":{"id":"cbc0517a"},"outputs":[],"source":["def add_hour_to_cont_date(x):\n","    if x['C_DATE']==datetime.datetime(1990, 1, 1):\n","        return x['C_DATE']\n","    total = x['C_DATE']\n","    if x['C_HOUR']>=0:\n","        total += datetime.timedelta(hours = int(x['CONT_TIME'][:2]), minutes=int(x['CONT_TIME'][2:]))\n","    return total "]},{"cell_type":"code","execution_count":null,"id":"542104d7","metadata":{"id":"542104d7"},"outputs":[],"source":["def duration(x):\n","    if x['C_DATE']==datetime.datetime(1990, 1, 1):\n","        return -1\n","    else:\n","        if x['EXISTS_TIMES']==1:\n","            difference = x['C_DATE']-x['DATE']\n","        else:\n","            difference = (x['C_DATE']+ datetime.timedelta(hours = 23, minutes=59))-x['DATE']  \n","        return difference"]},{"cell_type":"code","execution_count":null,"id":"54593db7","metadata":{"id":"54593db7"},"outputs":[],"source":["def duration_days(x):\n","    if x['DURATION']==-1:\n","        return -1\n","    return x['DURATION'].days"]},{"cell_type":"code","execution_count":null,"id":"f2470d60","metadata":{"id":"f2470d60"},"outputs":[],"source":["def duration_hours(x):\n","    if x['DURATION']==-1:\n","        return -1\n","    return x['DURATION'].days*24+x['DURATION'].seconds//3600"]},{"cell_type":"code","execution_count":null,"id":"eab746f2","metadata":{"id":"eab746f2"},"outputs":[],"source":["def duration_30m(x):\n","    if x['DURATION']==-1:\n","        return 0\n","    if x['DURATION']== datetime.timedelta(hours = 0, minutes=30):\n","        return 1\n","    return 0"]},{"cell_type":"code","execution_count":null,"id":"031914f7","metadata":{"id":"031914f7"},"outputs":[],"source":["def duration_1h(x):\n","    if x['DURATION']==-1:\n","        return 0\n","    if x['DURATION']== datetime.timedelta(hours = 1, minutes=0):\n","        return 1\n","    return 0"]},{"cell_type":"code","execution_count":null,"id":"478c7e7b","metadata":{"id":"478c7e7b"},"outputs":[],"source":["def duration_same_d(x):\n","    if x['DURATION_DAYS']==-1:\n","        return 0\n","    if x['DURATION_DAYS']==0:\n","        return 1\n","    return 0"]},{"cell_type":"code","execution_count":null,"id":"610de8ae","metadata":{"id":"610de8ae"},"outputs":[],"source":["def get_gdf(file_path):\n","        gdf = pd.read_csv(file_path)\n","        return gpd.GeoDataFrame(gdf.loc[:, [c for c in gdf.columns if c != \"geometry\"]], \n","                                geometry=gpd.GeoSeries.from_wkt(gdf[\"geometry\"]))"]},{"cell_type":"code","execution_count":null,"id":"68b1bac9","metadata":{"id":"68b1bac9"},"outputs":[],"source":["def get_distance_to_rails_feature(df):\n","    gdf = get_gdf(\"datasets/rail_north_america/rails_geo.csv\")\n","\n","    gdf.crs = \"EPSG:4326\"\n","\n","    gdf = gdf.to_crs(\"EPSG:3857\")\n","    df.crs = gdf.crs\n","    res = gpd.sjoin_nearest(df, gdf, distance_col=\"distance_to_rails\", how=\"left\")\n","    res = res.drop([\"index_right\", \"scalerank\", \"featurecla\", \"sov_a3\", \"uident\", \"add\", \"natrlscale\", \"continent\"], axis=1)\n","    return res"]},{"cell_type":"code","execution_count":null,"id":"1050a12d","metadata":{"id":"1050a12d"},"outputs":[],"source":["def get_pop(df):\n","    gdf = get_gdf(\"datasets/population_density/data_populations_usa.csv\")\n","    gdf.crs = \"EPSG:4326\"\n","\n","    gdf = gdf.to_crs(\"EPSG:3857\")\n","    \n","    res = gpd.sjoin_nearest(df, gdf, how=\"left\")\n","    res = res.drop([\"index_right\"], axis=1)\n","    \n","    return res"]},{"cell_type":"code","execution_count":null,"id":"7b39e3b1","metadata":{"id":"7b39e3b1"},"outputs":[],"source":["def get_distance_feature(df, file_path, feature_name):\n","    gdf = get_gdf(file_path)\n","\n","    gdf.crs = \"EPSG:4326\"\n","\n","    gdf = gdf.to_crs(\"EPSG:3857\")\n","    df.crs = gdf.crs\n","\n","    res = gpd.sjoin_nearest(df, gdf, distance_col=feature_name, how=\"left\")\n","    res = res.drop([\"OBJECTID\", \"index_right\"], axis=1)\n","    return res"]},{"cell_type":"code","execution_count":null,"id":"a7d0dfff","metadata":{"id":"a7d0dfff"},"outputs":[],"source":["def get_city_and_distance_feature(df, file_path, feature_name):\n","    gdf = get_gdf(file_path)\n","\n","    gdf.crs = \"EPSG:4326\"\n","\n","    gdf = gdf.to_crs(\"EPSG:3857\")\n","    df.crs = gdf.crs\n","\n","    res = gpd.sjoin_nearest(df, gdf, distance_col=feature_name, how=\"left\")\n","    res = res.drop([\"index_right\"], axis=1)\n","    return res"]},{"cell_type":"code","execution_count":null,"id":"23348d6e","metadata":{"id":"23348d6e"},"outputs":[],"source":["def date_to_check(row):\n","    day_to_check = row['DISCOVERY_DOY'] - (1 + (row['DISCOVERY_DOY'] - 1) % 3)\n","    date = datetime.datetime(row['YEAR'], 1, 1) + datetime.timedelta(int(day_to_check))\n","    \n","    if date.year < 1992:\n","        return 1992, 1, 1\n","    else:\n","        return date.year, date.month, date.day\n","\n","def temps_func(df):\n","    gdf = pd.read_csv(\"datasets/temps_dfs/temps_area_codes.csv\")\n","    gdf = gpd.GeoDataFrame(gdf.loc[:, [c for c in gdf.columns if c != \"geometry\"]],\n","                           geometry=gpd.GeoSeries.from_wkt(gdf[\"geometry\"]))\n","    \n","    gdf.crs = \"EPSG:4326\"\n","\n","    gdf = gdf.to_crs(\"EPSG:3857\")\n","    df.crs = gdf.crs\n","\n","    df = gpd.sjoin_nearest(df, gdf, how=\"left\")\n","    \n","    df = df.drop([\"index_right\"], axis=1)\n","\n","    df[\"day_remove\"] = df[\"DAY\"]\n","    df[\"month_remove\"] = df[\"MONTH\"]\n","    df[\"year_remove\"] = df[\"YEAR\"]\n","\n","    dates = np.array(list(df.apply(date_to_check, axis=1)))\n","\n","    df[\"DAY\"] = dates[:, 2]\n","    df[\"MONTH\"] = dates[:, 1]\n","    df[\"YEAR\"] = dates[:, 0]\n","\n","    gdf = pd.read_csv(f\"datasets/temps_dfs/temps_area_code_dates.csv\")\n","\n","    gdf[\"YEAR\"] = gdf[\"YEAR\"].astype(\"int32\")\n","    df[\"YEAR\"] = df[\"YEAR\"].astype(\"int32\")\n","    gdf[\"MONTH\"] = gdf[\"MONTH\"].astype(\"int32\")\n","    df[\"MONTH\"] = df[\"MONTH\"].astype(\"int32\")\n","    gdf[\"DAY\"] = gdf[\"DAY\"].astype(\"int32\")\n","    df[\"DAY\"] = df[\"DAY\"].astype(\"int32\")\n","\n","    df = pd.merge(df, gdf, on=[\"DAY\", \"MONTH\", \"YEAR\", \"area_code\"], how=\"left\")\n","\n","    df[\"DAY\"] = df[\"day_remove\"]\n","    df[\"MONTH\"] = df[\"month_remove\"]\n","    df[\"YEAR\"] = df[\"year_remove\"]\n","    df = df.drop([\"area_code\", \"day_remove\", \"month_remove\", \"year_remove\"], axis=1)\n","    df = df.drop_duplicates(subset=[\"FOD_ID\"])\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"id":"90638020","metadata":{"id":"90638020"},"outputs":[],"source":["us_state_to_abbrev = {\n","    \"Alabama\": \"AL\",\n","    \"Alaska\": \"AK\",\n","    \"Arizona\": \"AZ\",\n","    \"Arkansas\": \"AR\",\n","    \"California\": \"CA\",\n","    \"Colorado\": \"CO\",\n","    \"Connecticut\": \"CT\",\n","    \"Delaware\": \"DE\",\n","    \"Florida\": \"FL\",\n","    \"Georgia\": \"GA\",\n","    \"Hawaii\": \"HI\",\n","    \"Idaho\": \"ID\",\n","    \"Illinois\": \"IL\",\n","    \"Indiana\": \"IN\",\n","    \"Iowa\": \"IA\",\n","    \"Kansas\": \"KS\",\n","    \"Kentucky\": \"KY\",\n","    \"Louisiana\": \"LA\",\n","    \"Maine\": \"ME\",\n","    \"Maryland\": \"MD\",\n","    \"Massachusetts\": \"MA\",\n","    \"Michigan\": \"MI\",\n","    \"Minnesota\": \"MN\",\n","    \"Mississippi\": \"MS\",\n","    \"Missouri\": \"MO\",\n","    \"Montana\": \"MT\",\n","    \"Nebraska\": \"NE\",\n","    \"Nevada\": \"NV\",\n","    \"New Hampshire\": \"NH\",\n","    \"New Jersey\": \"NJ\",\n","    \"New Mexico\": \"NM\",\n","    \"New York\": \"NY\",\n","    \"North Carolina\": \"NC\",\n","    \"North Dakota\": \"ND\",\n","    \"Ohio\": \"OH\",\n","    \"Oklahoma\": \"OK\",\n","    \"Oregon\": \"OR\",\n","    \"Pennsylvania\": \"PA\",\n","    \"Rhode Island\": \"RI\",\n","    \"South Carolina\": \"SC\",\n","    \"South Dakota\": \"SD\",\n","    \"Tennessee\": \"TN\",\n","    \"Texas\": \"TX\",\n","    \"Utah\": \"UT\",\n","    \"Vermont\": \"VT\",\n","    \"Virginia\": \"VA\",\n","    \"Washington\": \"WA\",\n","    \"West Virginia\": \"WV\",\n","    \"Wisconsin\": \"WI\",\n","    \"Wyoming\": \"WY\",\n","    \"District of Columbia\": \"DC\",\n","    \"American Samoa\": \"AS\",\n","    \"Guam\": \"GU\",\n","    \"Northern Mariana Islands\": \"MP\",\n","    \"Puerto Rico\": \"PR\",\n","    \"Virgin Islands\": \"PR\",\n","    \"United States Minor Outlying Islands\": \"UM\",\n","    \"U.S. Virgin Islands\": \"VI\",\n","    \"United States\": \"United States\"\n","}\n","\n","def map_states(y):\n","    return us_state_to_abbrev.get(y, \"____\")\n","\n","# adds data from https://www.kff.org/statedata/\n","def get_info_by_states(df, address):\n","    df = df.sort_values(by=[\"YEAR\"])\n","    df[\"YEAR\"] = pd.to_numeric(df[\"YEAR\"])\n","\n","    csv_files = ['citenzship',\n","                 'race',\n","                 'tobacco',\n","                 'demographics_with_years']\n","    \n","    for table in csv_files:\n","        curr = pd.read_csv(f\"{address}/{table}.csv\")\n","        curr = curr.dropna()\n","        if table == 'tobacco':\n","            curr.rename(columns = {'State':'STATE', \"Year\":\"YEAR\"}, inplace = True)\n","        else:\n","            curr.rename(columns = {'Location':'STATE'}, inplace = True)\n","            \n","        curr['STATE'] = curr['STATE'].apply(map_states)\n","        \n","        if (table == 'tobacco') or (table == \"demographics_with_years\"):\n","            curr[\"YEAR\"] = pd.to_numeric(curr[\"YEAR\"])\n","            curr = curr.sort_values(by=[\"YEAR\"])\n","            \n","            df = pd.merge_asof(df, curr, on=\"YEAR\", by='STATE', direction=\"nearest\")\n","            \n","        else:\n","            df = pd.merge(df, curr, on=['STATE'], how=\"left\")\n","    \n","    df = df.drop_duplicates(subset=[\"FOD_ID\"])\n","    df = df.applymap(lambda x: x.strip('%') if isinstance(x, str) else x)\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"a42e8fcc","metadata":{"id":"a42e8fcc"},"outputs":[],"source":["def turn_fire_class_to_ordinal(row):\n","    if row[\"FIRE_SIZE_CLASS\"] == \"A\":\n","        return 0\n","    elif row[\"FIRE_SIZE_CLASS\"] == \"B\":\n","        return 1\n","    elif row[\"FIRE_SIZE_CLASS\"] == \"C\":\n","        return 2\n","    elif row[\"FIRE_SIZE_CLASS\"] == \"D\":\n","        return 3\n","    elif row[\"FIRE_SIZE_CLASS\"] == \"E\":\n","        return 4\n","    elif row[\"FIRE_SIZE_CLASS\"] == \"F\":\n","        return 5\n","    elif row[\"FIRE_SIZE_CLASS\"] == \"G\":\n","        return 6\n","    else:\n","        return -1"]},{"cell_type":"code","execution_count":null,"id":"df41ac15","metadata":{"id":"df41ac15"},"outputs":[],"source":["def get_features(df):\n","    df = df.to_crs(\"EPSG:3857\")\n","    print(\"getting pop feature...\", end=\"\")\n","    df = get_pop(df)\n","    df = df.groupby(\"FOD_ID\").first().reset_index()\n","    print(\"Done\")\n","    \n","    print(\"getting rails feature...\", end=\"\")\n","    df = get_distance_to_rails_feature(df)\n","    df = df.groupby(\"FOD_ID\").first().reset_index()\n","    print(\"Done\")\n","    \n","    print(\"getting powerline feature...\", end=\"\")\n","    df = get_distance_feature(df, \"datasets/powerlines/powerlines.csv\", \"distance_to_powerline\")\n","    df = df.groupby(\"FOD_ID\").first().reset_index()\n","    print(\"Done\")\n","    \n","    print(\"getting landfill feature...\", end=\"\")\n","    df = get_distance_feature(df, \"datasets/landfill_locations/Landfill_Locations.csv\", \"distance_to_landfill\")\n","    df = df.groupby(\"FOD_ID\").first().reset_index()\n","    print(\"Done\")\n","    \n","    print(\"getting home_parks_distance feature...\", end=\"\")\n","    df = get_distance_feature(df, \"datasets/mobile_home_parks/Mobile_Home_Parks.csv\", \"home_parks_distance\")\n","    df = df.groupby(\"FOD_ID\").first().reset_index()\n","    print(\"Done\")\n","    \n","    print(\"getting public school distance feature...\", end=\"\")\n","    df = get_distance_feature(df, \"datasets/schools/Public_Schools.csv\", \"public_school_distance\")\n","    df = df.groupby(\"FOD_ID\").first().reset_index()\n","    print(\"Done\")\n","    \n","    print(\"getting city and distance feature...\", end=\"\")\n","    df = get_city_and_distance_feature(df, \"datasets/cities/CityBoundaries.csv\", \"city_distance\")\n","    df = df.groupby(\"FOD_ID\").first().reset_index()\n","    print(\"Done\")\n","    \n","    print(\"getting temperature features...\", end=\"\")\n","    df = temps_func(df)\n","    print(\"Done\")\n","    \n","    print(\"getting ordinal fire class...\", end=\"\")\n","    df[\"fire_class_ordinal\"] = df.apply(turn_fire_class_to_ordinal, axis=1)\n","    print(\"Done\")\n","    \n","    print(\"getting usa by state features...\", end=\"\")\n","    df = get_info_by_states(df, \"datasets/usa_by_state\")\n","    print(\"Done\")\n","    \n","    df[\"max_temp\"].fillna(value=df[\"max_temp\"].mean(), inplace=True)\n","    df[\"min_temp\"].fillna(value=df[\"min_temp\"].mean(), inplace=True)\n","    df[\"prcp\"].fillna(value=df[\"prcp\"].mean(), inplace=True)\n","    df[\"Pan_evaporation\"].fillna(value=df[\"Pan_evaporation\"].mean(), inplace=True)\n","    df[\"avg_temp\"].fillna(value=df[\"avg_temp\"].mean(), inplace=True)\n","    \n","    df = df.fillna(0)\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"id":"29276844","metadata":{"id":"29276844"},"outputs":[],"source":["def preprocess(df, encoder):\n","    print(\"working on dates...\", end=\"\")\n","    df['DATE']=df.apply(to_date, axis=1)\n","    df['CONT_DATE'] = df.apply(to_date, axis=1)\n","    \n","    df['DATE'] = pd.to_datetime(df['DATE'])\n","    df['DAY'] = df['DATE'].dt.day\n","    df['MONTH'] = df['DATE'].dt.month\n","    df['YEAR'] = df['DATE'].dt.year\n","    \n","    df['JULY4']=df.apply(july4, axis=1)\n","    \n","    df['C_DATE'] = df.apply(to_cont_date, axis=1)\n","    df['C_DATE_EXISTS'] = df.apply(is_cont_date, axis=1)\n","    df['C_DATE'] = pd.to_datetime(df['C_DATE'])\n","    df['EXISTS_TIMES']=df.apply(exists_times, axis=1)\n","    df['HOUR']=df.apply(add_hour, axis=1)\n","    df['DATE']=df.apply(add_hour_do_dis_date, axis=1)\n","    df['C_HOUR']=df.apply(add_cont_hour, axis=1)\n","    df['C_DATE']=df.apply(add_hour_to_cont_date, axis=1)\n","    df['DURATION']=df.apply(duration, axis=1)\n","    df['DURATION_DAYS']=df.apply(duration_days, axis=1)\n","    df['DURATION_HOURS']=df.apply(duration_hours, axis=1)\n","    df['DUR_30M']=df.apply(duration_30m, axis=1)\n","    df['DUR_1H']=df.apply(duration_1h, axis=1)\n","    df['DUR_1D']=df.apply(duration_same_d, axis=1)\n","    print(\"Done\")\n","    \n","    df = df.drop(columns=[\"FPA_ID\", \"SOURCE_SYSTEM_TYPE\", \"SOURCE_SYSTEM\", \"NWCG_REPORTING_AGENCY\",\n","                         \"NWCG_REPORTING_UNIT_ID\", \"NWCG_REPORTING_UNIT_NAME\", \"SOURCE_REPORTING_UNIT\",\n","                         \"SOURCE_REPORTING_UNIT_NAME\", \"Shape\", \"OBJECTID\", \"LOCAL_FIRE_REPORT_ID\",\n","                         \"LOCAL_INCIDENT_ID\", \"FIRE_CODE\", \"FIRE_NAME\", \"ICS_209_INCIDENT_NUMBER\",\n","                         \"ICS_209_NAME\", \"MTBS_ID\", \"MTBS_FIRE_NAME\", \"COMPLEX_NAME\", \"FIPS_CODE\", \"FIPS_NAME\",\n","                         \"OWNER_CODE\", \"DATE\", \"FIRE_YEAR\", \"DISCOVERY_DATE\", \"DISCOVERY_TIME\", \n","                         \"CONT_DATE\", \"CONT_DOY\", \"CONT_TIME\", \"C_DATE\", \"DURATION\"])\n","    \n","    df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.LONGITUDE, df.LATITUDE), crs=\"EPSG:4326\")\n","    \n","    df = get_features(df)\n","    \n","    df = df.rename(columns={\"z\": \"population density\"})\n","    df = df.drop(columns=[\"FOD_ID\", \"geometry\"])\n","    \n","    print(\"One hot encoding...\", end=\"\")\n","    COLUMNS_NAME_ONE_HOT = list(encoder.categories_[0]) + list(encoder.categories_[1]) + list(encoder.categories_[2])\n","    one_hots = pd.DataFrame(encoder.transform(df[[\"STATE\", \"OWNER_DESCR\", \"NAME\"]]),\n","                            index=df.index, columns=COLUMNS_NAME_ONE_HOT)\n","    print(\"Done\")\n","\n","    df = pd.concat([df, one_hots], axis=1)\n","\n","    df = df.drop(columns=[\"COUNTY\", \"STATE\", \"FIRE_SIZE_CLASS\", \"OWNER_DESCR\", \"NAME\", \"Unnamed: 0\"])\n","    \n","    features = [i for i in df.columns if i not in ['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR']]\n","    df[features] = df[features].astype(\"float\")\n","    \n","    print(\"Interaction of smokers and population size...\", end=\"\")\n","    df[\"Smoke everyday number\"] = df[\"Smoke everyday\"] * df[\"Total\"]\n","    df[\"Smoke some days number\"] = df[\"Smoke some days\"] * df[\"Total\"]\n","    df[\"Former smoker number\"] = df[\"Former smoker\"] * df[\"Total\"]\n","    df[\"Never smoked number\"] = df[\"Never smoked\"] * df[\"Total\"]\n","    print(\"Done\")\n","    \n","    features = [i for i in df.columns if i not in ['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR']]\n","    \n","    print(\"Normalizing data...\", end=\"\")\n","    scaler = StandardScaler()\n","    df_normalized = pd.DataFrame(scaler.fit_transform(df[features]), columns=[features])\n","    df[features] = df_normalized\n","    print(\"Done\")\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"id":"da335638","metadata":{"id":"da335638"},"outputs":[],"source":["class ConfusionMatrixMixtureOfExperts:\n","    def __init__(self, initial_model, initial_model_params, distance_threshold):\n","        self.initial_model = initial_model(**initial_model_params)\n","        self.clusters = None\n","        self.class2expert_classifier = {}\n","        self.distance_threshold = distance_threshold\n","        self.label2idx = {}\n","        self.idx2label = {}\n","        self.class2cluster = None\n","        self.cluster2classes = {}\n","    \n","    def get_labels(self, y_train):\n","        for idx, label in enumerate(set(y_train)):\n","            self.idx2label[idx] = label\n","            self.label2idx[label] = idx\n","            \n","    def choose_clf(self, X, y):\n","        X_train, X_test, y_train, y_test = train_test_split(X, y)\n","        best_clf = None\n","        best_f1 = 0\n","        # random forest:\n","        max_depth = [10, 20, 30, 40, 50]\n","        for depth in max_depth:\n","            rf = RandomForestClassifier(max_depth=depth, n_jobs=-1)\n","            rf.fit(X_train, y_train)\n","            f1 = f1_score(y_test, rf.predict(X_test), average='weighted')\n","            if f1 > best_f1:\n","                best_clf = rf\n","                best_f1 = f1\n","        return best_clf\n","    \n","    def fit(self, X, y):\n","        self.get_labels(y)\n","        y = np.array(list(map(self.label2idx.get, y)))\n","        self.initial_model.fit(X, y)\n","        y_pred = self.initial_model.predict(X)\n","        cm = confusion_matrix(y_pred, y)\n","        dists = 1 - cm\n","        cm = (cm + cm.transpose()) / 2\n","        self.class2cluster = AgglomerativeClustering(n_clusters=None, linkage='average', distance_threshold=self.distance_threshold,\n","                                                     affinity='precomputed').fit_predict(dists)\n","        # self.class2cluster[i] is cluster of class i\n","        for class_, cluster in enumerate(self.class2cluster):\n","            classes = self.cluster2classes.get(cluster, [])\n","            self.cluster2classes[cluster] = classes + [class_]\n","        # self.cluster2classes[c] is classes belonging to cluster c\n","        print(self.cluster2classes)\n","        print(f\"found {len(set(self.class2cluster))} cluster\")\n","        for cluster, classes in self.cluster2classes.items():\n","            if len(classes) > 1:\n","                # selecting examples with class value in clsuter\n","                X_cluster = X[[y_ in classes for y_ in y]]  \n","                y_cluster = y[[y_ in classes for y_ in y]]\n","                # training an expert for classes\n","                expert = self.choose_clf(X_cluster, y_cluster)\n","                print(f\"fitting expert for cluster {cluster}\")\n","                expert.fit(X_cluster, y_cluster)\n","                for y_ in classes:\n","                    self.class2expert_classifier[y_] = expert\n","            else:\n","                # the cluster contains a single class, no need for expert\n","                y_ = classes[0]\n","                self.class2expert_classifier[y_] = None\n","                \n","        return self\n","\n","    def predict(self, X):\n","        y_pred_initial = self.initial_model.predict(X)\n","        y_pred_final = y_pred_initial.copy()\n","        \n","        for i, y_init in enumerate(y_pred_initial):\n","            # getting expert for predicted class, and predicting with expert\n","            expert = self.class2expert_classifier[y_init]\n","            if expert is not None:\n","                y = expert.predict([X[i]])[0]\n","                y_pred_final[i] = y\n","                \n","        y_pred_final = np.array(list(map(self.idx2label.get, y_pred_final)))\n","                \n","        return y_pred_final"]},{"cell_type":"code","execution_count":null,"id":"1b4e0988","metadata":{"id":"1b4e0988"},"outputs":[],"source":["def test_model_on_test_set(test_df):\n","    print(\"Preprocessing test df.\")\n","    encoder = pickle.load(open(\"encoder.p\", \"rb\" ))\n","    test_df = preprocess(test_df, encoder)\n","    \n","    X_test = test_df.drop(columns=['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR'])\n","    y_test = test_df['STAT_CAUSE_DESCR']\n","    \n","    X_test = np.array(X_test)\n","    y_test = np.array(y_test)\n","    \n","    \n","    print(\"Training our chosen models:\")\n","    df = pd.read_csv('datasets/preprocessed_final.csv')\n","    \n","    X = df.drop(columns=['STAT_CAUSE_CODE', 'STAT_CAUSE_DESCR'])\n","    y = df['STAT_CAUSE_DESCR']\n","    \n","    print(\"Using random over sampler...\", end=\"\")\n","    ros = RandomOverSampler(random_state=0)\n","    X, y = ros.fit_resample(X, y)\n","    print(\"Done.\")\n","    \n","    print(\"Training Random forest:\")\n","    rf = RandomForestClassifier(random_state= 666, n_estimators= 400, min_samples_split= 2, min_samples_leaf= 2,\n","                                max_features= 'sqrt', max_depth= 55, bootstrap= True, n_jobs=-1)\n","    \n","    rf.fit(X,y)\n","    print(\"Predicting labels of test set using random forest...\", end=\"\")\n","    preds_rf = rf.predict(X_test)\n","    print(\"Done.\")\n","    \n","    print(classification_report(y_test, preds_rf))\n","    \n","    print(\"Training Confusion Matrix Mixture of Experts:\")\n","    model = ConfusionMatrixMixtureOfExperts(RandomForestClassifier, {\"n_jobs\":-1}, distance_threshold=0.9)\n","    model.fit(X, y)\n","    \n","    print(\"Predicting labels of test set...\", end=\"\")\n","    preds_conf = model.predict(X_test)\n","    print(\"Done.\")\n","    \n","    print(classification_report(y_test, preds_conf))\n","    \n","    return X_test, y_test, preds_conf, preds_rf"]},{"cell_type":"markdown","id":"466d241a","metadata":{"id":"466d241a"},"source":["The block below runs the model on the test_df provided above, it returns the preprocessed X and y from the test df and the predictions returned by the model.\n","The Confusion Matrix Mixture of Experts might take awhile to finish predicting, which is why we first present results acquired through RandomForestClassifier using parameters found using random search cv."]},{"cell_type":"code","execution_count":null,"id":"3dc9d026","metadata":{"id":"3dc9d026","outputId":"a423a10b-a6fe-4b4a-9419-ac4b0fba9091"},"outputs":[{"name":"stdout","output_type":"stream","text":["Preprocessing test df.\n","working on dates...Done\n","getting pop feature...Done\n","getting rails feature...Done\n","getting powerline feature...Done\n","getting landfill feature...Done\n","getting home_parks_distance feature...Done\n","getting public school distance feature...Done\n","getting city and distance feature...Done\n","getting temperature features...Done\n","getting ordinal fire class...Done\n","getting usa by state features...Done\n","One hot encoding...Done\n","Interaction of smokers and population size...Done\n","Normalizing data...Done\n","Training our chosen models:\n","Using random over sampler...Done.\n","Training Random forest:\n","Predicting labels of test set using random forest...Done.\n","                   precision    recall  f1-score   support\n","\n","Missing/Undefined       1.00      1.00      1.00         1\n","\n","         accuracy                           1.00         1\n","        macro avg       1.00      1.00      1.00         1\n","     weighted avg       1.00      1.00      1.00         1\n","\n","Training Confusion Matrix Mixture of Experts:\n","{1: [0], 0: [1]}\n","found 2 cluster\n","Predicting labels of test set...Done.\n","                   precision    recall  f1-score   support\n","\n","Missing/Undefined       1.00      1.00      1.00         1\n","\n","         accuracy                           1.00         1\n","        macro avg       1.00      1.00      1.00         1\n","     weighted avg       1.00      1.00      1.00         1\n","\n"]}],"source":["X_test, y_test, preds_conf, preds_rf = test_model_on_test_set(test_df)"]},{"cell_type":"code","execution_count":null,"id":"a73e65e6","metadata":{"id":"a73e65e6"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}