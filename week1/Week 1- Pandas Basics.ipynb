{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll be going through some examples and uses of basic Python tools for handling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "% matplotlib inline\n",
    "\n",
    "np.set_printoptions(suppress=True) ##Suppress scientific notation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we'll be using might be familiar for some of you- we'll be taking a look at the flight delay data used at the IML hackathon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to read the data into memory.\n",
    "Other than reading CSV files, Pandas can read other file formats, data through API calls, use SQL to connect and read from database tables, parse tables from HTML and even read data from your clipboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything fancy, let's start by taking a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have several ways to simply take a look at our dataset before handling it in a more thorough way. When we'll delve more deeply into the EDA process we'll look at some more ways to do so.\n",
    "\n",
    "As you might know, Pandas is built on top of Numpy arrays. In case we want to start messing with the underlying array, we can call: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, when .info() was called, we saw each column's datatype. Take note of it- handling large datasets can take a toll on your memory usage and performance, and making sure the correct Dtype for each column is being used is a good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have some data in-memory. What do we do with it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing our dataframe can be done in two main way. Using location indices with the .loc and .iloc properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:2,['DayOfWeek','Origin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And accessing columns on a named basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Origin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Origin\",\"DayOfWeek\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate functions and filtering, take 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so we can select columns. But what now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we wanted to take a closer look at the different origins. Specifically,\n",
    "we wanted to see which airports are 'busy'. How should we define this question more succintly so that we can try and answer this through our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate functions are functions which are applied to different parts of a given dataset. Let's take a look so it becomes clearer. Say we wanted to check how many flights took off from JFK- as it's a major NYC airport, we assume it's probably a good example of a busy airport. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we could do is just filter our dataset to flights originating from JFK, and look at the resulting dataframe's shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Origin\"]==\"JFK\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, 10394 records are of flight from JFK, with our 17 columns of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to ponder if this is a scaleable way to do things. If later down the line we wanted to use this as a feature in our project, should we do this for every airport?\n",
    "\n",
    "Considering i wrote \"take 1\" in our title, you can safely assume \"no\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate functions, take 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, say we wanted to get a count for every airport around. \n",
    "Applying an aggragate function is essentialy grouping our data by specific properties and applying some sort of function- like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Origin\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we got 16 equal columns- the .count methods did not distinguish between the existing columns. It also turned our index to the Origin columns.\n",
    "In order to return to a previous,more easy-to-use format, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Origin\").count()['DayOfWeek'].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a single column, denoted here as \"DayOfWeek\", simply lets us choose one of the 16 identical columns- any existing colum will do. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should note that the amount columns grouped by is arbitrary- we could group by any set of columns (as long as they exist). We just have to make sure that whatever we're doing makes sense:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Origin\",\"Dest\"]).count()['DayOfWeek'].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a lot of rows to look at. At this point, we could consider visualizing our data to gain better insights. Let's try this now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like most over tasks, Python has a plethora of data viz libraries:\n",
    "- Matplotlib (which we've seen at previous courses)\n",
    "- Seaborn, which is written on top of matplotlib and can usually get a nicer looking baseline with less of a hassle\n",
    "- Plotly, which allows interactive visualization\n",
    "\n",
    "and many, many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look- we'll import seaborn and try and make some visualization to better understand what's going on with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_plot= df.groupby(\"Origin\").count()['DayOfWeek'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"DayOfWeek\",data=to_plot,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that wasn't really to informative- our data is highly concentrated and it's a bit hard to comprehend what's going on. \n",
    "Let's use this opportunity to see how we apply functions and transformations to our data in a more basic, non-aggregate way.\n",
    "One possible way to improve our visualization is to apply a log transformation to our count column. Note that this lowers the explainability of our plot a bit, but it's fine (for now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could, for example, create a new column to preserve our current existing data. Note that as pandas DataFrames and Series are based on numpy objects, we could make use of its implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot['log_days']=np.log( to_plot.DayOfWeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"log_days\",data=to_plot,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is applying a method axis- wise. For example, lets try normalizing our dataset to have a mean of 0 and sd of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot['normalized_count']=to_plot['DayOfWeek']  -to_plot['DayOfWeek'].mean()  \n",
    "to_plot['normalized_count'] =to_plot['normalized_count'] / to_plot['DayOfWeek'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot['normalized_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot['normalized_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"normalized_count\",data=to_plot,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Merging data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's talk about using more than a single dataset.\n",
    "For example, let's load up our provided weather data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv(\"all_weather_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might remember, this dataset contains weather conditions for different stations on different days.\n",
    "What we're going to do is join together our two dataframes to one larger and more informative one.\n",
    "Joins, which some of you might recognize from SQL, define the process of merging (or... joining) two datasets together.\n",
    "The two main types of joins are Inner and Outer joins, with outer joins have several subtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dofactory.com/Images/sql/sql-joins.png\" width=500 height=500 />\n",
    "<center>How did we get this far without Venn diagrams?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how is a join performed and what's the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When declaring a join, we use what we'd call keys. We're going to look for corresponding columns in both joined tables, which have corresponding values, and 'join' using them. Don't worry, we'll take a look at it in a minute.\n",
    "The types of joins are as follows:\n",
    "\n",
    "- Inner join: Only keep rows with matches in both tables. This means that, for example, if we had an airport without any weather data or weather data for a non tracked airport (or worse, for a wrongly written airport name) - we'll lose it when joining. This is not necessarily a bad thing- but something to note.\n",
    "- Outer join: Allow missing values on at least one side. A left or right join allows no-matches on the relvant side, and a full join allows them on both tables.\n",
    "\n",
    "\n",
    "Pandas does joining using the .merge method. We'll perform our example join using the origin station and day. First, we'll convert our Date columns to datetime, and perform our join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.FlightDate=pd.to_datetime(df.FlightDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.day=pd.to_datetime(weather_df.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(weather_df,left_on=['Origin',\"FlightDate\"],right_on = ['station','day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yielding a much richer dataset for us to work with!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}