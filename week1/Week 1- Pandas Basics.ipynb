{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll be going through some examples and uses of basic Python tools for handling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "% matplotlib inline\n",
    "\n",
    "np.set_printoptions(suppress=True) ##Suppress scientific notation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we'll be using might be familiar for some of you- we'll be taking a look at the flight delay data used at the IML hackathon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to read the data into memory.\n",
    "Other than reading CSV files, Pandas can read other file formats, data through API calls, use SQL to connect and read from database tables, parse tables from HTML and even read data from your clipboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything fancy, let's start by taking a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   DayOfWeek  FlightDate Reporting_Airline Tail_Number  \\\n0          5  2018-05-18                WN      N218WN   \n1          5  2017-07-21                OO      N243SY   \n2          2  2011-04-05                XE      N41104   \n3          4  2019-09-12                DL      N325NB   \n4          4  2016-02-11                F9      N926FR   \n\n   Flight_Number_Reporting_Airline Origin    OriginCityName OriginState Dest  \\\n0                              921    BNA     Nashville, TN          TN  SAN   \n1                             4688    SEA       Seattle, WA          WA  SAN   \n2                             3051    JAX  Jacksonville, FL          FL  IAH   \n3                             1496    LGA      New York, NY          NY  MSP   \n4                             1184    MKE     Milwaukee, WI          WI  MCO   \n\n      DestCityName DestState  CRSDepTime  CRSArrTime  CRSElapsedTime  \\\n0    San Diego, CA        CA      1850.0      2115.0           265.0   \n1    San Diego, CA        CA       945.0      1244.0           179.0   \n2      Houston, TX        TX       600.0       712.0           132.0   \n3  Minneapolis, MN        MN      1035.0      1245.0           190.0   \n4      Orlando, FL        FL       645.0      1050.0           185.0   \n\n   Distance  ArrDelay   DelayFactor  \n0    1751.0      29.0  WeatherDelay  \n1    1050.0      -4.0           NaN  \n2     817.0      98.0  WeatherDelay  \n3    1020.0     -16.0           NaN  \n4    1066.0      29.0  WeatherDelay  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DayOfWeek</th>\n      <th>FlightDate</th>\n      <th>Reporting_Airline</th>\n      <th>Tail_Number</th>\n      <th>Flight_Number_Reporting_Airline</th>\n      <th>Origin</th>\n      <th>OriginCityName</th>\n      <th>OriginState</th>\n      <th>Dest</th>\n      <th>DestCityName</th>\n      <th>DestState</th>\n      <th>CRSDepTime</th>\n      <th>CRSArrTime</th>\n      <th>CRSElapsedTime</th>\n      <th>Distance</th>\n      <th>ArrDelay</th>\n      <th>DelayFactor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>2018-05-18</td>\n      <td>WN</td>\n      <td>N218WN</td>\n      <td>921</td>\n      <td>BNA</td>\n      <td>Nashville, TN</td>\n      <td>TN</td>\n      <td>SAN</td>\n      <td>San Diego, CA</td>\n      <td>CA</td>\n      <td>1850.0</td>\n      <td>2115.0</td>\n      <td>265.0</td>\n      <td>1751.0</td>\n      <td>29.0</td>\n      <td>WeatherDelay</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>2017-07-21</td>\n      <td>OO</td>\n      <td>N243SY</td>\n      <td>4688</td>\n      <td>SEA</td>\n      <td>Seattle, WA</td>\n      <td>WA</td>\n      <td>SAN</td>\n      <td>San Diego, CA</td>\n      <td>CA</td>\n      <td>945.0</td>\n      <td>1244.0</td>\n      <td>179.0</td>\n      <td>1050.0</td>\n      <td>-4.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2011-04-05</td>\n      <td>XE</td>\n      <td>N41104</td>\n      <td>3051</td>\n      <td>JAX</td>\n      <td>Jacksonville, FL</td>\n      <td>FL</td>\n      <td>IAH</td>\n      <td>Houston, TX</td>\n      <td>TX</td>\n      <td>600.0</td>\n      <td>712.0</td>\n      <td>132.0</td>\n      <td>817.0</td>\n      <td>98.0</td>\n      <td>WeatherDelay</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2019-09-12</td>\n      <td>DL</td>\n      <td>N325NB</td>\n      <td>1496</td>\n      <td>LGA</td>\n      <td>New York, NY</td>\n      <td>NY</td>\n      <td>MSP</td>\n      <td>Minneapolis, MN</td>\n      <td>MN</td>\n      <td>1035.0</td>\n      <td>1245.0</td>\n      <td>190.0</td>\n      <td>1020.0</td>\n      <td>-16.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2016-02-11</td>\n      <td>F9</td>\n      <td>N926FR</td>\n      <td>1184</td>\n      <td>MKE</td>\n      <td>Milwaukee, WI</td>\n      <td>WI</td>\n      <td>MCO</td>\n      <td>Orlando, FL</td>\n      <td>FL</td>\n      <td>645.0</td>\n      <td>1050.0</td>\n      <td>185.0</td>\n      <td>1066.0</td>\n      <td>29.0</td>\n      <td>WeatherDelay</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 539877 entries, 0 to 539876\n",
      "Data columns (total 17 columns):\n",
      " #   Column                           Non-Null Count   Dtype  \n",
      "---  ------                           --------------   -----  \n",
      " 0   DayOfWeek                        539877 non-null  int64  \n",
      " 1   FlightDate                       539877 non-null  object \n",
      " 2   Reporting_Airline                539877 non-null  object \n",
      " 3   Tail_Number                      539877 non-null  object \n",
      " 4   Flight_Number_Reporting_Airline  539877 non-null  int64  \n",
      " 5   Origin                           539877 non-null  object \n",
      " 6   OriginCityName                   539877 non-null  object \n",
      " 7   OriginState                      539877 non-null  object \n",
      " 8   Dest                             539877 non-null  object \n",
      " 9   DestCityName                     539877 non-null  object \n",
      " 10  DestState                        539877 non-null  object \n",
      " 11  CRSDepTime                       539877 non-null  float64\n",
      " 12  CRSArrTime                       539877 non-null  float64\n",
      " 13  CRSElapsedTime                   539877 non-null  float64\n",
      " 14  Distance                         539877 non-null  float64\n",
      " 15  ArrDelay                         539877 non-null  float64\n",
      " 16  DelayFactor                      269960 non-null  object \n",
      "dtypes: float64(5), int64(2), object(10)\n",
      "memory usage: 70.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "           DayOfWeek  Flight_Number_Reporting_Airline     CRSDepTime  \\\ncount  539877.000000                    539877.000000  539877.000000   \nmean        3.922306                      2337.517562    1360.221497   \nstd         1.990011                      1859.690165     477.304919   \nmin         1.000000                         1.000000       1.000000   \n25%         2.000000                       768.000000     943.000000   \n50%         4.000000                      1803.000000    1355.000000   \n75%         6.000000                      3683.000000    1745.000000   \nmax         7.000000                      8880.000000    2359.000000   \n\n          CRSArrTime  CRSElapsedTime       Distance       ArrDelay  \ncount  539877.000000   539877.000000  539877.000000  539877.000000  \nmean     1526.005172      140.073743     805.452062      26.898964  \nstd       505.076949       74.532696     601.114446      69.281464  \nmin         1.000000      -99.000000      31.000000    -102.000000  \n25%      1129.000000       85.000000     364.000000     -11.000000  \n50%      1554.000000      120.000000     640.000000      15.000000  \n75%      1936.000000      170.000000    1040.000000      40.000000  \nmax      2400.000000      718.000000    4983.000000    2692.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DayOfWeek</th>\n      <th>Flight_Number_Reporting_Airline</th>\n      <th>CRSDepTime</th>\n      <th>CRSArrTime</th>\n      <th>CRSElapsedTime</th>\n      <th>Distance</th>\n      <th>ArrDelay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>539877.000000</td>\n      <td>539877.000000</td>\n      <td>539877.000000</td>\n      <td>539877.000000</td>\n      <td>539877.000000</td>\n      <td>539877.000000</td>\n      <td>539877.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.922306</td>\n      <td>2337.517562</td>\n      <td>1360.221497</td>\n      <td>1526.005172</td>\n      <td>140.073743</td>\n      <td>805.452062</td>\n      <td>26.898964</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.990011</td>\n      <td>1859.690165</td>\n      <td>477.304919</td>\n      <td>505.076949</td>\n      <td>74.532696</td>\n      <td>601.114446</td>\n      <td>69.281464</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>-99.000000</td>\n      <td>31.000000</td>\n      <td>-102.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000</td>\n      <td>768.000000</td>\n      <td>943.000000</td>\n      <td>1129.000000</td>\n      <td>85.000000</td>\n      <td>364.000000</td>\n      <td>-11.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>4.000000</td>\n      <td>1803.000000</td>\n      <td>1355.000000</td>\n      <td>1554.000000</td>\n      <td>120.000000</td>\n      <td>640.000000</td>\n      <td>15.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.000000</td>\n      <td>3683.000000</td>\n      <td>1745.000000</td>\n      <td>1936.000000</td>\n      <td>170.000000</td>\n      <td>1040.000000</td>\n      <td>40.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.000000</td>\n      <td>8880.000000</td>\n      <td>2359.000000</td>\n      <td>2400.000000</td>\n      <td>718.000000</td>\n      <td>4983.000000</td>\n      <td>2692.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have several ways to simply take a look at our dataset before handling it in a more thorough way. When we'll delve more deeply into the EDA process we'll look at some more ways to do so.\n",
    "\n",
    "As you might know, Pandas is built on top of Numpy arrays. In case we want to start messing with the underlying array, we can call: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, when .info() was called, we saw each column's datatype. Take note of it- handling large datasets can take a toll on your memory usage and performance, and making sure the correct Dtype for each column is being used is a good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have some data in-memory. What do we do with it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing our dataframe can be done in two main way. Using location indices with the .loc and .iloc properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:2,['DayOfWeek','Origin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And accessing columns on a named basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Origin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Origin\",\"DayOfWeek\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate functions and filtering, take 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so we can select columns. But what now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we wanted to take a closer look at the different origins. Specifically,\n",
    "we wanted to see which airports are 'busy'. How should we define this question more succintly so that we can try and answer this through our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate functions are functions which are applied to different parts of a given dataset. Let's take a look so it becomes clearer. Say we wanted to check how many flights took off from JFK- as it's a major NYC airport, we assume it's probably a good example of a busy airport. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we could do is just filter our dataset to flights originating from JFK, and look at the resulting dataframe's shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Origin\"]==\"JFK\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, 10394 records are of flight from JFK, with our 17 columns of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to ponder if this is a scaleable way to do things. If later down the line we wanted to use this as a feature in our project, should we do this for every airport?\n",
    "\n",
    "Considering i wrote \"take 1\" in our title, you can safely assume \"no\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate functions, take 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, say we wanted to get a count for every airport around. \n",
    "Applying an aggragate function is essentialy grouping our data by specific properties and applying some sort of function- like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Origin\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we got 16 equal columns- the .count methods did not distinguish between the existing columns. It also turned our index to the Origin columns.\n",
    "In order to return to a previous,more easy-to-use format, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Origin\").count()['DayOfWeek'].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a single column, denoted here as \"DayOfWeek\", simply lets us choose one of the 16 identical columns- any existing colum will do. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should note that the amount columns grouped by is arbitrary- we could group by any set of columns (as long as they exist). We just have to make sure that whatever we're doing makes sense:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Origin\",\"Dest\"]).count()['DayOfWeek'].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a lot of rows to look at. At this point, we could consider visualizing our data to gain better insights. Let's try this now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like most over tasks, Python has a plethora of data viz libraries:\n",
    "- Matplotlib (which we've seen at previous courses)\n",
    "- Seaborn, which is written on top of matplotlib and can usually get a nicer looking baseline with less of a hassle\n",
    "- Plotly, which allows interactive visualization\n",
    "\n",
    "and many, many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look- we'll import seaborn and try and make some visualization to better understand what's going on with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_plot= df.groupby(\"Origin\").count()['DayOfWeek'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_2920/2908060101.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0msns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mboxplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"DayOfWeek\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mto_plot\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "sns.boxplot(y=\"DayOfWeek\",data=to_plot,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that wasn't really to informative- our data is highly concentrated and it's a bit hard to comprehend what's going on. \n",
    "Let's use this opportunity to see how we apply functions and transformations to our data in a more basic, non-aggregate way.\n",
    "One possible way to improve our visualization is to apply a log transformation to our count column. Note that this lowers the explainability of our plot a bit, but it's fine (for now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could, for example, create a new column to preserve our current existing data. Note that as pandas DataFrames and Series are based on numpy objects, we could make use of its implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot['log_days']=np.log( to_plot.DayOfWeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"log_days\",data=to_plot,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is applying a method axis- wise. For example, lets try normalizing our dataset to have a mean of 0 and sd of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot['normalized_count']=to_plot['DayOfWeek']  -to_plot['DayOfWeek'].mean()  \n",
    "to_plot['normalized_count'] =to_plot['normalized_count'] / to_plot['DayOfWeek'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot['normalized_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot['normalized_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"normalized_count\",data=to_plot,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Merging data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's talk about using more than a single dataset.\n",
    "For example, let's load up our provided weather data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv(\"all_weather_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might remember, this dataset contains weather conditions for different stations on different days.\n",
    "What we're going to do is join together our two dataframes to one larger and more informative one.\n",
    "Joins, which some of you might recognize from SQL, define the process of merging (or... joining) two datasets together.\n",
    "The two main types of joins are Inner and Outer joins, with outer joins have several subtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dofactory.com/Images/sql/sql-joins.png\" width=500 height=500 />\n",
    "<center>How did we get this far without Venn diagrams?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how is a join performed and what's the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When declaring a join, we use what we'd call keys. We're going to look for corresponding columns in both joined tables, which have corresponding values, and 'join' using them. Don't worry, we'll take a look at it in a minute.\n",
    "The types of joins are as follows:\n",
    "\n",
    "- Inner join: Only keep rows with matches in both tables. This means that, for example, if we had an airport without any weather data or weather data for a non tracked airport (or worse, for a wrongly written airport name) - we'll lose it when joining. This is not necessarily a bad thing- but something to note.\n",
    "- Outer join: Allow missing values on at least one side. A left or right join allows no-matches on the relvant side, and a full join allows them on both tables.\n",
    "\n",
    "\n",
    "Pandas does joining using the .merge method. We'll perform our example join using the origin station and day. First, we'll convert our Date columns to datetime, and perform our join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.FlightDate=pd.to_datetime(df.FlightDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.day=pd.to_datetime(weather_df.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(weather_df,left_on=['Origin',\"FlightDate\"],right_on = ['station','day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yielding a much richer dataset for us to work with!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}